# Note-Long-tailed-learning


​	This project is a review of the paper that I have read. Those paper for intensive reading will be marked with ''*'' , meanwhile, I'll      submit the corresponding note in the format of markdown or slide if the paper will be shared during my group report.

​	The contents of each paper's review are  presented as following: Title, authors, Publication, Submission time,    Citations, Paper link, Code link, Note(or slides) link. The ''Submission time'' is the first time that the author submit the paper in arxiv.


<br>

## Survey
---

- ##### Long-Tailed Classification by Keeping the Good  and Removing the Bad Momentum Causal Effect<br>

  Authors: Kaihua Tang(NTU), Hanwang Zhang(NTU)<br>
  Publication: NeurIPS-2020<br>
  Submission time:(first version in arxiv): 28 Sep 2020<br>
  Citations: 94<br>
  [Paper link](<https://arxiv.org/abs/2009.12991>)<br>
  [Code link](<https://github.com/Zcchill/Awesome-LongTailed-Learning>)<br>
  My Note(or slides) link: [Note(md)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/Deep%20Long-Tailed%20Learning-A%20Survey.pdf)<br>


<br>

## Rebalanced-learning
---

- ##### Long-Tailed Classification by Keeping the Good  and Removing the Bad Momentum Causal Effect<br>

  Authors: Kaihua Tang(NTU), Hanwang Zhang(NTU)<br>
  Publication: NeurIPS-2020<br>
  Submission time:(first version in arxiv): 28 Sep 2020<br>
  Citations: 94<br>
  [Paper link](<https://arxiv.org/abs/2009.12991>)<br>
  [Code link (official code)](<https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch>)<br>
  My Note(or slides) link: [Presentation(slides)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/NeurIPS-2020-Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect.pdf)<br>
  
  

- ##### Long tail learning via logit adjustment 

  Authors: Aditya Krishna Menon (Google)<br>
  Publication: ICLR-2021<br>
  Submission time:(first version in arxiv): 14 Jul 2020<br>
  Citations: 70<br>
  [Paper link](<https://arxiv.org/abs/2007.07314>)<br>
  [Code link (official code, TF )](<https://github.com/google-research/google-research/tree/master/logit_adjustment>)<br>
  [Code link (Community code, pytorch):](<https://github.com/Chumsy0725/logit-adj-pytorch>)
  
  My Note(or slides) link: [Notes(md)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/ICLR-2021-Long%20tail%20learning%20via%20logit%20adjustment.pdf)<br>
  
  

<br>

## Information augmentation
---

- ##### Rethinking the Value of Labels for Improving Class-Imbalanced Learning

  Authors: Yuzhe Yang(MIT)<br>
  Publication: NeurlPS-2020<br>
  Submission time:(first version in arxiv): 13 Jun 2020<br>
  Citations: 70<br>
  [Paper link](<https://arxiv.org/abs/2006.07529>)<br>
  [Code link (official code)](https://github.com/YyzHarry/imbalanced-semi-self)<br>

  My Note(or slides) link: [Presentation(slides)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/NeurIPS-2020-rethinking-the-value-of-labels-for-improving-class-imbalanced-learning-Paper.pdf)<br>



<br>

## Module Improvement
---

- ##### Decoupling representation and classifier for long-tailed recognition

  Authors: Bingyi Kang(Facebook)<br>
  Publication: ICLR-2020<br>
  Submission time:(first version in arxiv): 21 Oct 2019<br>
  Citations: 234<br>
  [Paper link](<https://arxiv.org/abs/1910.09217>)<br>
  [Code link (official code)](<https://github.com/facebookresearch/classifier-balancing>)<br>

  My Note(or slides) link: [Presentation(slides)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/ICLR-2020-DECOUPLING%20REPRESENTATION%20AND%20CLASSIFIER.pdf)<br>



- ##### Long-tailed Recognition by Routing Diverse  Distribution-Aware Experts

  Authors: Xudong Wang (University of California, Berkeley)<br>
  Publication: ICLR-2021<br>
  Submission time:(first version in arxiv): 5 Oct 2020<br>
  Citations: 39<br>
  [Paper link](<https://arxiv.org/abs/2010.01809>)<br>
  [Code link (official code)](<https://github.com/frank-xwang/RIDE-LongTailRecognition>)<br>

  My Note(or slides) link: [Presentation(slides)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/ICLR-2021-LONG-TAILED%20RECOGNITION%20BY%20ROUTING%20DIVERSE.pdf)<br>

<br>

## Other reletive paper (not directly concerned with long-tail learning, but is useful to solve this problem)
---

- ##### Implicit Semantic Data Augmentation for Deep Networks

  Authors: Yulin Wang,Gao Huang(Tsinghua)<br>
  Publication: NeurlPS-2019<br>
  Submission time:(first version in arxiv): 26 Sep 2019<br>
  Citations: 42<br>
  [Paper link](<https://arxiv.org/abs/1909.12220>)<br>
  [Code link (Official code)](<https://github.com/blackfeather-wang/ISDA-for-Deep-Networks>)<br>

  My Note(or slides) link: [Note(md)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/NeurlPS-2019-Implicit-Semantic-Data-Augmentation-for-Deep-Networks.pdf)<br>

- ##### Attention Is All You Need

  Authors: Ashish Vaswani(Facebook)<br>
  Publication: NeurlPS-2017<br>
  Submission time:(first version in arxiv): 12 Jun 2017<br>
  Citations: 9999<br>
  [Paper link](<https://arxiv.org/abs/1706.03762>)<br>
  [Code link (Community code, pytorch)](<https://github.com/graykode/nlp-tutorial>)<br>

  My Note(or slides) link: [Presentation(slides)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/Attention_Is_All_You_Need.pptx)<br>


- ##### Training Generative Adversarial Networks with Limited Data

  Authors: Tero Karras (Nvidia)<br>
  Publication: NeurlPS-2020<br>
  Submission time:(first version in arxiv): 11 Jun 2020<br>
  Citations: 334<br>
  [Paper link](<https://arxiv.org/abs/2006.06676>)<br>
  [Code link (official code)](<https://github.com/NVlabs/stylegan2-ada>)<br>

  My Note(or slides) link: [Presentation(slides)](https://github.com/Zcchill/Note-Long-tailed-learning/blob/main/Note%20and%20Presentations/NeurlPS2020-Training%20Generative%20Adversarial%20Networks%20with.pdf)<br>
